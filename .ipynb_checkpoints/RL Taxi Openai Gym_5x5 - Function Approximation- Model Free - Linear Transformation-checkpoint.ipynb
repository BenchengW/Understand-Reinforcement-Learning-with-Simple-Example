{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (3.20.2)\n",
      "Requirement already satisfied: gym in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from gym) (1.17.4)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\admin\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake gym scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "import numpy as np\n",
    "\n",
    "MAP = [\n",
    "    \"+---------+\",\n",
    "    \"|R: | : :G|\",\n",
    "    \"| : | : : |\",\n",
    "    \"| : : : : |\",\n",
    "    \"| | : | : |\",\n",
    "    \"|Y| : |B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "\n",
    "\n",
    "# taxi_row, taxi_col, pass_idx, dest_idx = 3, \n",
    "# taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "\n",
    "\n",
    "class TaxiEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    The Taxi Problem\n",
    "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "    Description:\n",
    "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "    Observations:\n",
    "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
    "    Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "    Rewards:\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +20,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.desc = np.asarray(MAP, dtype='c')\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
    "\n",
    "        num_states = 500\n",
    "        num_rows = 5\n",
    "        num_columns = 5\n",
    "        max_row = num_rows - 1\n",
    "        max_col = num_columns - 1\n",
    "        initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "        P = {state: {action: []\n",
    "                     for action in range(num_actions)} for state in range(num_states)}\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_columns):\n",
    "                for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "                    for dest_idx in range(len(locs)):\n",
    "                        state = self.encode(row, col, pass_idx, dest_idx)\n",
    "                        if pass_idx < 4 and pass_idx != dest_idx:\n",
    "                            initial_state_distrib[state] += 1\n",
    "                        for action in range(num_actions):\n",
    "                            # defaults\n",
    "                            new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                            reward = -1  # default reward when there is no pickup/dropoff\n",
    "                            done = False\n",
    "                            taxi_loc = (row, col)\n",
    "\n",
    "                            if action == 0:\n",
    "                                new_row = min(row + 1, max_row)\n",
    "                            elif action == 1:\n",
    "                                new_row = max(row - 1, 0)\n",
    "                            if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                                new_col = min(col + 1, max_col)\n",
    "                            elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n",
    "                                new_col = max(col - 1, 0)\n",
    "                            elif action == 4:  # pickup\n",
    "                                if (pass_idx < 4 and taxi_loc == locs[pass_idx]):\n",
    "                                    new_pass_idx = 4\n",
    "                                else:  # passenger not at location\n",
    "                                    reward = -10\n",
    "                            elif action == 5:  # dropoff\n",
    "                                if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n",
    "                                    new_pass_idx = dest_idx\n",
    "                                    done = True\n",
    "                                    reward = 20\n",
    "                                elif (taxi_loc in locs) and pass_idx == 4:\n",
    "                                    new_pass_idx = locs.index(taxi_loc)\n",
    "                                else:  # dropoff at wrong location\n",
    "                                    reward = -10\n",
    "                            new_state = self.encode(\n",
    "                                new_row, new_col, new_pass_idx, dest_idx)\n",
    "                            P[state][action].append(\n",
    "                                (1.0, new_state, reward, done))\n",
    "        initial_state_distrib /= initial_state_distrib.sum()\n",
    "        discrete.DiscreteEnv.__init__(\n",
    "            self, num_states, num_actions, P, initial_state_distrib)\n",
    "\n",
    "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "        # (5) 5, 5, 4\n",
    "        i = taxi_row\n",
    "        i *= 5\n",
    "        i += taxi_col\n",
    "        i *= 5\n",
    "        i += pass_loc\n",
    "        i *= 4\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        out = []\n",
    "        out.append(i % 4)\n",
    "        i = i // 4\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 5\n",
    "        return reversed(out)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        out = self.desc.copy().tolist()\n",
    "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "\n",
    "        def ul(x): return \"_\" if x == \" \" else x\n",
    "        if pass_idx < 4:\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                out[1 + taxi_row][2 * taxi_col + 1], 'yellow', highlight=True)\n",
    "            pi, pj = self.locs[pass_idx]\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
    "        else:  # passenger in taxi\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        # No need to return anything for human\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 243\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = TaxiEnv()\n",
    "state = env.encode(2, 2, 0, 3)\n",
    "print(\"State:\", state)\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "lb.transform([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpisodes = 1000\n",
    "discountFactor = 1\n",
    "alpha = 0.01\n",
    "nA = env.action_space.n\n",
    "\n",
    "w = np.zeros((nA,100))\n",
    "epRewards = np.zeros(numEpisodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epRewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 100)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 243\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = 243 # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_examples = np.array([list(env.decode(env.observation_space.sample())) for x in range(1000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=20)),\n",
       "                               ('rbf2', RBFSampler(gamma=2.0, n_components=20)),\n",
       "                               ('rbf3', RBFSampler(n_components=20)),\n",
       "                               ('rbf4', RBFSampler(gamma=0.5, n_components=20)),\n",
       "                               ('rbf5',\n",
       "                                RBFSampler(gamma=0.5, n_components=20))])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=20)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=20)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=20)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=20)),\n",
    "        (\"rbf5\", RBFSampler(gamma=0.5, n_components=20)),\n",
    "        ])\n",
    "\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 3, 0],\n",
       "       [2, 4, 1, 0],\n",
       "       [3, 0, 4, 3],\n",
       "       ...,\n",
       "       [0, 1, 3, 1],\n",
       "       [4, 4, 4, 1],\n",
       "       [1, 2, 4, 3]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_feature = list(env.decode(232))\n",
    "# # state_feature\n",
    "# scaler.transform([state_feature])\n",
    "# A = featurizer.transform(scaler.transform([state_feature]))\n",
    "# A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_state(state):\n",
    "    scaled = scaler.transform([state])\n",
    "    #featurized = featurizer.transform(scaled)\n",
    "    return scaled\n",
    "\n",
    "def policy(state, weight, epsilon=0.5):\n",
    "    A = np.ones(nA,dtype=float) * epsilon/nA\n",
    "    best_action =  np.argmax([state.dot(w[a]) for a in range(nA)])\n",
    "    A[best_action] += (1.0-epsilon)\n",
    "    sample = np.random.choice(nA,p=A)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "all_rewards = []\n",
    "\n",
    "for e in range(numEpisodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, cum_reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    count = 0 \n",
    "    while not done:\n",
    "        \n",
    "        state_feature = list(env.decode(state))\n",
    "        state_feature = featurize_state(state_feature)\n",
    "        action = policy(state_feature,w)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        next_state_feature = list(env.decode(next_state))\n",
    "        next_state_feature = featurize_state(next_state_feature)\n",
    "        \n",
    "        next_action = policy(next_state_feature,w)\n",
    "        \n",
    "        epRewards[e] += reward\n",
    "        \n",
    "        target = reward + discountFactor * next_state_feature.dot(w[next_action])\n",
    "        td_error = state_feature.dot(w[action]) - target\n",
    "\n",
    "        dw = (td_error).dot(state_feature)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        w[action] -= alpha * dw\n",
    "\n",
    "        cum_reward += reward\n",
    "        epochs += 1\n",
    "        \n",
    "        count +=1\n",
    "        if count >=5000:\n",
    "            break\n",
    "        \n",
    "    all_epochs.append(epochs)\n",
    "    all_penalties.append(penalties)\n",
    "    all_rewards.append(cum_reward)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"Episode: {}\".format(e))\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>epochs_to_complete</th>\n",
       "      <th>rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9168</td>\n",
       "      <td>-22845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2772</td>\n",
       "      <td>-6297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>16666</td>\n",
       "      <td>-41350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3184</td>\n",
       "      <td>-7663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>11708</td>\n",
       "      <td>-29174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10233</td>\n",
       "      <td>-25044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>3345</td>\n",
       "      <td>-8346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5520</td>\n",
       "      <td>-13716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>61607</td>\n",
       "      <td>-153287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>589</td>\n",
       "      <td>-1783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>22208</td>\n",
       "      <td>-55559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>2732</td>\n",
       "      <td>-6977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode  epochs_to_complete  rewards\n",
       "0         1                9168   -22845\n",
       "1         2                2772    -6297\n",
       "2         3               16666   -41350\n",
       "3         4                3184    -7663\n",
       "4         5               11708   -29174\n",
       "5         6               10233   -25044\n",
       "6         7                3345    -8346\n",
       "7         8                5520   -13716\n",
       "8         9               61607  -153287\n",
       "9        10                 589    -1783\n",
       "10       11               22208   -55559\n",
       "11       12                2732    -6977"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tuples = list(zip(list(range(1, 999)), all_epochs, all_rewards))\n",
    "learning_df = pd.DataFrame(data_tuples, columns=['episode','epochs_to_complete', 'rewards'])\n",
    "learning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10152032e-01, -9.71559971e-01,  5.85536051e+00,\n",
       "        -5.99425533e+02],\n",
       "       [-1.31932947e-01, -9.87866252e-01,  5.80903221e+00,\n",
       "        -5.99468542e+02],\n",
       "       [-1.43683196e-01, -9.93196132e-01,  5.75525249e+00,\n",
       "        -5.99705071e+02],\n",
       "       [-1.35183773e-01, -9.38246897e-01,  5.91348655e+00,\n",
       "        -5.99483197e+02],\n",
       "       [-4.60403215e-01, -5.82930322e-01,  4.67784438e+00,\n",
       "        -6.03225298e+02],\n",
       "       [ 7.83535948e-01, -1.21879389e+00,  6.21780156e+00,\n",
       "        -6.03082166e+02]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-da37c07fefa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstate_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mstate_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-098a5a4e38da>\u001b[0m in \u001b[0;36mpolicy\u001b[1;34m(state, weight, epsilon)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mbest_action\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "rewards = []\n",
    "\n",
    "state = 243\n",
    "env.s = state\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    state_feature = list(env.decode(state))\n",
    "    state_feature = featurize_state(state_feature)\n",
    "    action = policy(state_feature,w)\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    rewards.append(reward)\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-0210a5a83f5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(\"Timestep: {}\".format(i + 1))\n",
    "        print(\"State: {}\".format(frame['state']))\n",
    "        print(\"Action: {}\".format(frame['action']))\n",
    "        print(\"Reward: {}\".format(frame['reward']))\n",
    "        sleep(0.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11456669,  0.28164057, -0.09517834,  0.19615317, -0.269186  ,\n",
       "        -0.12565423,  0.27832016,  0.25468262, -0.17450584,  0.28181043,\n",
       "         0.28269382,  0.27171563, -0.12153166, -0.12417229,  0.24794124,\n",
       "        -0.26318235,  0.17458221,  0.28274556,  0.10750719, -0.2749872 ,\n",
       "        -0.27676149,  0.06463112,  0.17141615, -0.27738014,  0.05040022]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_state([state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
